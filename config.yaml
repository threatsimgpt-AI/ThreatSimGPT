# Production Configuration for ThreatSimGPT

# LLM Provider Configuration
llm:
  # Default provider selection (openai, anthropic, openrouter, ollama, huggingface, llamacpp)
  default_provider: "openrouter"
  
  # Primary provider - OpenAI
  openai:
    api_key: ""  # Set via environment variable OPENAI_API_KEY
    model: "gpt-4o-mini"  # Cost-effective model for production
    base_url: "https://api.openai.com/v1"
    max_tokens: 1000
    temperature: 0.7
    timeout_seconds: 60
    retry_attempts: 3
  
  # Secondary provider - Anthropic Claude
  anthropic:
    api_key: ""  # Set via environment variable ANTHROPIC_API_KEY
    model: "claude-3-haiku-20240307"  # Fast and cost-effective
    base_url: "https://api.anthropic.com"
    max_tokens: 1000
    temperature: 0.7
    timeout_seconds: 60
    retry_attempts: 3
  
  # Recommended provider - OpenRouter (access to multiple models)
  openrouter:
    api_key: ""  # Set via environment variable OPENROUTER_API_KEY
    model: "openai/gpt-5.1-chat"  # Default model
    base_url: "https://openrouter.ai/api/v1"
    app_name: "ThreatSimGPT"
    site_url: "https://github.com/ThreatSimGPT/ThreatSimGPT"
    max_tokens: 3000
    temperature: 0.7
    timeout_seconds: 60
    retry_attempts: 3
    
    # Popular models available via OpenRouter
    available_models:
      - "anthropic/claude-3-haiku"           # Fast, cost-effective
      - "anthropic/claude-3-sonnet"          # Balanced performance  
      - "openai/gpt-4o-mini"                 # Cost-effective GPT-4
      - "openai/gpt-4o"                      # High performance
      - "meta-llama/llama-3.1-70b-instruct" # Open source, powerful
      - "meta-llama/llama-3.1-8b-instruct"  # Open source, fast
      - "mistralai/mixtral-8x7b-instruct"    # Efficient performance
      - "google/gemini-pro"                  # Google's flagship model
  
  # Local LLM provider - Ollama (offline, no API key required)
  ollama:
    enabled: true
    base_url: "http://localhost:11434"  # Default Ollama server URL
    model: "llama2"  # Default model (can be changed)
    timeout_seconds: 120  # Longer timeout for local generation
    max_tokens: 1000
    temperature: 0.7
    
    # Popular models you can use with Ollama
    # To install: ollama pull <model-name>
    recommended_models:
      - "llama2"           # Meta's Llama 2 (7B) - Good all-rounder
      - "llama2:13b"       # Llama 2 13B - Better quality
      - "mistral"          # Mistral 7B - Fast and capable
      - "codellama"        # Code-specialized Llama
      - "neural-chat"      # Intel's neural chat model
      - "starling-lm"      # Starling - Good for chat
      - "vicuna"           # Vicuna - ChatGPT alternative
      - "orca-mini"        # Orca Mini - Compact and fast
  # Local Model Providers (for privacy, cost control, and offline usage)
  
  # Ollama - Server-based local models (recommended for most users)
  ollama:
    enabled: true  # Set to true to enable
    base_url: "http://localhost:11434"
    model: "llama3.2:1b"  # Default model to load
    timeout_seconds: 120
    max_tokens: 1000
    temperature: 0.7
    
    # Popular models for cybersecurity training
    recommended_models:
      - "llama3.2:3b"      # Fast, good for basic scenarios
      - "llama3.2:1b"      # Very fast, minimal resources
      - "mistral:7b"       # Balanced performance
      - "codellama:7b"     # Code-aware for technical scenarios
      - "phi3:3.8b"        # Efficient instruction following
      - "qwen2:7b"         # Good reasoning capabilities
  
  # Hugging Face Transformers - Direct model loading with GPU support
  huggingface:
    enabled: false  # Set to true to enable
    model_name_or_path: "microsoft/DialoGPT-medium"
    model_type: "causal"  # "causal" or "seq2seq"
    cache_dir: "./models/huggingface"
    device_map: "auto"  # "auto", "cpu", or specific GPU
    torch_dtype: "auto"  # "auto", "float16", "float32"
    load_in_8bit: false  # Enable 8-bit quantization
    load_in_4bit: false  # Enable 4-bit quantization (requires bitsandbytes)
    use_cache: true
    trust_remote_code: false
    
    # Generation settings
    max_tokens: 1000
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    repetition_penalty: 1.1
    
    # Recommended models by use case
    recommended_models:
      small_fast: ["microsoft/DialoGPT-small", "distilgpt2", "gpt2"]
      medium_balanced: ["microsoft/DialoGPT-medium", "gpt2-medium"]
      code_specialized: ["Salesforce/codegen-350M-multi", "microsoft/CodeGPT-small-py"]
  
  # llama.cpp - High-performance CPU inference with GGUF support
  llamacpp:
    enabled: false  # Set to true to enable
    model_path: ""  # Path to GGUF/GGML model file (required)
    n_ctx: 2048     # Context length
    n_threads: null # CPU threads (auto-detect if null)
    n_batch: 512    # Batch size for processing
    n_gpu_layers: 0 # Number of layers to run on GPU (0 = CPU only)
    
    # Memory and performance
    use_mmap: true   # Memory-map model file
    use_mlock: false # Lock model in memory
    low_vram: false  # Reduce VRAM usage
    f16_kv: true     # Use half precision for key-value cache
    
    # Generation settings
    max_tokens: 1000
    temperature: 0.7
    seed: -1  # Random seed (-1 for random)
    
    # Example model downloads (set model_path after downloading)
    recommended_downloads:
      small_fast: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf"
      medium_balanced: "https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.q4_0.gguf"

# Core Simulation Settings
simulation:
  max_stages: 10
  default_timeout_minutes: 30
  enable_safety_checks: true
  enable_content_filtering: true
  enable_audit_logging: true
  max_concurrent_simulations: 5

# Safety and Compliance
safety:
  enable_safety_validation: true
  blocked_keywords:
    - "real malware"
    - "actual exploit"
    - "live attack"
  content_moderation: true
  compliance_mode: true
  audit_all_operations: true

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_path: "logs/threatsimgpt.log"
  max_file_size_mb: 50
  backup_count: 5
  enable_console_logging: true

# Template Configuration
templates:
  directory: "templates"
  validate_on_load: true
  cache_templates: true
  auto_reload: false

# Dataset Configuration
datasets:
  storage_path: "~/.threatsimgpt/datasets"
  enron:
    enabled: true
    description: "Enron email corpus for business communication patterns"
    update_interval: "monthly"
  phishtank:
    enabled: true
    description: "PhishTank database for phishing URL patterns"
    update_interval: "weekly"
  cert_insider:
    enabled: true
    description: "CERT insider threat database for behavioral patterns"
    update_interval: "monthly"
  lanl_auth:
    enabled: true
    description: "LANL authentication logs for network behavior analysis"
    update_interval: "monthly"
  mitre_attack:
    enabled: true
    description: "MITRE ATT&CK framework for tactics, techniques, and procedures"
    update_interval: "weekly"

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  debug: false
  cors_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"
  rate_limiting:
    enabled: true
    requests_per_minute: 60
  authentication:
    enabled: false  # Set to true for production
    api_key: ""  # Set via environment variable API_KEY

# Deployment Configuration (Optional - for advanced features)
deployment:
  email:
    smtp_server: ""
    smtp_port: 587
    smtp_username: ""
    smtp_password: ""  # Set via environment variable
    use_tls: true
  
  # Enterprise platform integrations (all optional)
  integrations:
    microsoft365:
      tenant_id: ""
      client_id: ""
      client_secret: ""  # Set via environment variable
    
    proofpoint:
      api_key: ""  # Set via environment variable
      base_url: "https://tap-api-v2.proofpoint.com"
    
    knowbe4:
      api_key: ""  # Set via environment variable
      base_url: "https://us.api.knowbe4.com"

# Intelligence/OSINT Configuration (Optional)
intelligence:
  enable_osint: false
  sources:
    linkedin: false
    company_research: false
    threat_intel: false
  rate_limits:
    requests_per_hour: 100
  privacy:
    respect_robots_txt: true
    user_agent: "ThreatSimGPT-Research/1.0"

# Database Configuration (Optional - for advanced features)
database:
  enabled: false
  url: "sqlite:///threatsimgpt.db"  # Can be PostgreSQL, MySQL, etc.
  pool_size: 5
  max_overflow: 10